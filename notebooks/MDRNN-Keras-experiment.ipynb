{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Input, merge\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "from tensorflow.contrib.distributions import Categorical, Mixture, MultivariateNormalDiag\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import h5py\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from context import * # imports robojam\n",
    "# import robojam # alternatively do this.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "input_colour = 'darkblue'\n",
    "gen_colour = 'firebrick'\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_df_to_array(perf_df):\n",
    "    \"\"\"Converts a dataframe of a performance into array a,b,dt format.\"\"\"\n",
    "    perf_df['dt'] = perf_df.time.diff()\n",
    "    perf_df.dt = perf_df.dt.fillna(0.0)\n",
    "    # Clean performance data\n",
    "    # Tiny Performance bounds defined to be in [[0,1],[0,1]], edit to fix this.\n",
    "    perf_df.set_value(perf_df[perf_df.dt > 5].index, 'dt', 5.0)\n",
    "    perf_df.set_value(perf_df[perf_df.dt < 0].index, 'dt', 0.0)\n",
    "    perf_df.set_value(perf_df[perf_df.x > 1].index, 'x', 1.0)\n",
    "    perf_df.set_value(perf_df[perf_df.x < 0].index, 'x', 0.0)\n",
    "    perf_df.set_value(perf_df[perf_df.y > 1].index, 'y', 1.0)\n",
    "    perf_df.set_value(perf_df[perf_df.y < 0].index, 'y', 0.0)\n",
    "    return np.array(perf_df[['x', 'y', 'dt']])\n",
    "\n",
    "\n",
    "def perf_array_to_df(perf_array):\n",
    "    \"\"\"Converts an array of a performance (a,b,dt format) into a dataframe.\"\"\"\n",
    "    perf_array = perf_array.T\n",
    "    perf_df = pd.DataFrame({'x': perf_array[0], 'y': perf_array[1], 'dt': perf_array[2]})\n",
    "    perf_df['time'] = perf_df.dt.cumsum()\n",
    "    perf_df['z'] = 38.0\n",
    "    # As a rule of thumb, could classify taps with dt>0.1 as taps, dt<0.1 as moving touches.\n",
    "    perf_df['moving'] = 1\n",
    "    perf_df.set_value(perf_df[perf_df.dt > 0.1].index, 'moving', 0)\n",
    "    perf_df = perf_df.set_index(['time'])\n",
    "    return perf_df[['x', 'y', 'z', 'moving']]\n",
    "\n",
    "\n",
    "def random_touch():\n",
    "    \"\"\"Generate a random tiny performance touch.\"\"\"\n",
    "    return np.array([np.random.rand(), np.random.rand(), 0.01])\n",
    "\n",
    "\n",
    "def constrain_touch(touch):\n",
    "    \"\"\"Constrain touch values from the MDRNN\"\"\"\n",
    "    touch[0] = min(max(touch[0], 0.0), 1.0)  # x in [0,1]\n",
    "    touch[1] = min(max(touch[1], 0.0), 1.0)  # y in [0,1]\n",
    "    touch[2] = max(touch[2], 0.001)  # dt # define minimum time step\n",
    "    return touch\n",
    "\n",
    "def generate_random_tiny_performance(model, first_touch, time_limit=5.0, steps_limit=1000, temp=1.0):\n",
    "    \"\"\"Generates a tiny performance up to 5 seconds in length.\"\"\"\n",
    "    time = 0\n",
    "    steps = 0\n",
    "    previous_touch = first_touch\n",
    "    performance = [previous_touch.reshape((3,))]\n",
    "    while (steps < steps_limit and time < time_limit):\n",
    "        previous_touch = model.predict(previous_touch.reshape(1,1,3))\n",
    "        output_touch = previous_touch.reshape(3,)\n",
    "        output_touch = constrain_touch(output_touch)\n",
    "        performance.append(output_touch.reshape((3,)))\n",
    "        steps += 1\n",
    "        time += output_touch[2]\n",
    "    return np.array(performance)\n",
    "\n",
    "\n",
    "def condition_and_generate(model, perf, n_mixtures, time_limit=5.0, steps_limit=1000, temp=1.0):\n",
    "    \"\"\"Conditions the network on an existing tiny performance, then generates a new one.\"\"\"\n",
    "    time = 0\n",
    "    steps = 0\n",
    "    # condition\n",
    "    for touch in perf:\n",
    "        params = model.predict(touch.reshape(1,1,3))\n",
    "        previous_touch = sample_from_output(params[0], n_mixtures, 3, temp=temp)\n",
    "    output = [previous_touch.reshape((3,))]\n",
    "    while (steps < steps_limit and time < time_limit):\n",
    "        params = model.predict(previous_touch.reshape(1,1,3))\n",
    "        previous_touch = sample_from_output(params[0], n_mixtures, 3, temp=temp)\n",
    "        output_touch = previous_touch.reshape(3,)\n",
    "        output_touch = constrain_touch(output_touch)\n",
    "        output.append(output_touch.reshape((3,)))\n",
    "        steps += 1\n",
    "        time += output_touch[2]\n",
    "    net_output = np.array(output)\n",
    "    return net_output\n",
    "\n",
    "def divide_performance_into_swipes(perf_df):\n",
    "    \"\"\"Divides a performance into a sequence of swipe dataframes for plotting.\"\"\"\n",
    "    touch_starts = perf_df[perf_df.moving == 0].index\n",
    "    performance_swipes = []\n",
    "    remainder = perf_df\n",
    "    for att in touch_starts:\n",
    "        swipe = remainder.iloc[remainder.index < att]\n",
    "        performance_swipes.append(swipe)\n",
    "        remainder = remainder.iloc[remainder.index >= att]\n",
    "    performance_swipes.append(remainder)\n",
    "    return performance_swipes\n",
    "\n",
    "def plot_2D(perf_df, name=\"foo\", saving=False):\n",
    "    \"\"\"Plot a 2D representation of a performance 2D\"\"\"\n",
    "    swipes = divide_performance_into_swipes(perf_df)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=gen_colour, linewidth=5.0)\n",
    "    plt.ylim(1.0,0)\n",
    "    plt.xlim(0,1.0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if saving:\n",
    "        plt.savefig(name+\".png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "def plot_double_2d(perf1, perf2, name=\"foo\", saving=False):\n",
    "    \"\"\"Plot two performances in 2D\"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    swipes = divide_performance_into_swipes(perf1)\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=input_colour, linewidth=5.0)\n",
    "    swipes = divide_performance_into_swipes(perf2)\n",
    "    for swipe in swipes:\n",
    "        p = plt.plot(swipe.x, swipe.y, 'o-')\n",
    "        plt.setp(p, color=gen_colour, linewidth=5.0)\n",
    "    plt.ylim(1.0,0)\n",
    "    plt.xlim(0,1.0)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    if saving:\n",
    "        plt.savefig(name+\".png\", bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture Density Layer in Keras\n",
    "\n",
    "See [https://github.com/cpmpercussion/keras-mdn-layer](https://github.com/cpmpercussion/keras-mdn-layer) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.engine.topology import Layer\n",
    "import numpy as np\n",
    "from tensorflow.contrib.distributions import Categorical, Mixture, MultivariateNormalDiag\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def elu_plus_one_plus_epsilon(x):\n",
    "    \"\"\"ELU activation with a very small addition to help prevent NaN in loss.\"\"\"\n",
    "    return (K.elu(x) + 1 + 1e-8)\n",
    "\n",
    "\n",
    "class MDN(Layer):\n",
    "    \"\"\"A Mixture Density Network Layer for Keras.\n",
    "    This layer has a few tricks to avoid NaNs in the loss function when training:\n",
    "        - Activation for variances is ELU + 1 + 1e-8 (to avoid very small values)\n",
    "        - Mixture weights (pi) are trained in as logits, not in the softmax space.\n",
    "\n",
    "    A loss function needs to be constructed with the same output dimension and number of mixtures.\n",
    "    A sampling function is also provided to sample from distribution parametrised by the MDN outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dim, num_mix, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.num_mix = num_mix\n",
    "        with tf.name_scope('MDN'):\n",
    "            self.mdn_mus = Dense(self.num_mix * self.output_dim, name='mdn_mus')  # mix*output vals, no activation\n",
    "            self.mdn_sigmas = Dense(self.num_mix * self.output_dim, activation=elu_plus_one_plus_epsilon, name='mdn_sigmas')  # mix*output vals exp activation\n",
    "            self.mdn_pi = Dense(self.num_mix, name='mdn_pi')  # mix vals, logits\n",
    "        super(MDN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.mdn_mus.build(input_shape)\n",
    "        self.mdn_sigmas.build(input_shape)\n",
    "        self.mdn_pi.build(input_shape)\n",
    "        self.trainable_weights = self.mdn_mus.trainable_weights + self.mdn_sigmas.trainable_weights + self.mdn_pi.trainable_weights\n",
    "        self.non_trainable_weights = self.mdn_mus.non_trainable_weights + self.mdn_sigmas.non_trainable_weights + self.mdn_pi.non_trainable_weights\n",
    "        super(MDN, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        with tf.name_scope('MDN'):\n",
    "            mdn_out = keras.layers.concatenate([self.mdn_mus(x),\n",
    "                                                self.mdn_sigmas(x),\n",
    "                                                self.mdn_pi(x)],\n",
    "                                               name='mdn_outputs')\n",
    "        return mdn_out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "\n",
    "def get_mixture_loss_func(output_dim, num_mixes):\n",
    "    \"\"\"Construct a loss functions for the MDN layer parametrised by number of mixtures.\"\"\"\n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def loss_func(y_true, y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                         num_mixes * output_dim,\n",
    "                                                                         num_mixes],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        mixture = Mixture(cat=cat, components=coll)\n",
    "        loss = mixture.log_prob(y_true)\n",
    "        loss = tf.negative(loss)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDN'):\n",
    "        return loss_func\n",
    "\n",
    "\n",
    "def get_mixture_sampling_fun(output_dim, num_mixes):\n",
    "    \"\"\"Construct a sampling function for the MDN layer parametrised by mixtures and output dimension.\"\"\"\n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def sampling_func(y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                         num_mixes * output_dim,\n",
    "                                                                         num_mixes],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        mixture = Mixture(cat=cat, components=coll)\n",
    "        samp = mixture.sample()\n",
    "        # Todo: temperature adjustment for sampling function.\n",
    "        return samp\n",
    "\n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "        return sampling_func\n",
    "\n",
    "\n",
    "def get_mixture_mse_accuracy(output_dim, num_mixes):\n",
    "    \"\"\"Construct an MSE accuracy function for the MDN layer\n",
    "    that takes one sample and compares to the true value.\"\"\"\n",
    "    # Construct a loss function with the right number of mixtures and outputs\n",
    "    def mse_func(y_true, y_pred):\n",
    "        out_mu, out_sigma, out_pi = tf.split(y_pred, num_or_size_splits=[num_mixes * output_dim,\n",
    "                                                                         num_mixes * output_dim,\n",
    "                                                                         num_mixes],\n",
    "                                             axis=1, name='mdn_coef_split')\n",
    "        cat = Categorical(logits=out_pi)\n",
    "        component_splits = [output_dim] * num_mixes\n",
    "        mus = tf.split(out_mu, num_or_size_splits=component_splits, axis=1)\n",
    "        sigs = tf.split(out_sigma, num_or_size_splits=component_splits, axis=1)\n",
    "        coll = [MultivariateNormalDiag(loc=loc, scale_diag=scale) for loc, scale\n",
    "                in zip(mus, sigs)]\n",
    "        mixture = Mixture(cat=cat, components=coll)\n",
    "        samp = mixture.sample()\n",
    "        mse = tf.reduce_mean(tf.square(samp - y_true), axis=-1)\n",
    "        # Todo: temperature adjustment for sampling functon.\n",
    "        return mse\n",
    "\n",
    "    # Actually return the loss_func\n",
    "    with tf.name_scope('MDNLayer'):\n",
    "        return mse_func\n",
    "\n",
    "\n",
    "# # Sample from Categorical distribution\n",
    "# def sample_mixture(mus, sigs, pis):\n",
    "#     \"\"\"Sample from a mixture of 1D normal distributions parameterised by mus, sigma, and pi.\"\"\"\n",
    "#     m = np.random.choice(range(len(pis)), p=pis)\n",
    "#     return(np.random.normal(mus[m],sigs[m],1))\n",
    "\n",
    "def split_mixture_params(params, mixtures, dim):\n",
    "    \"\"\"Splits up an array of mixture parameters into mus, sigmas, and pis\n",
    "    depending on the number of mixtures and output dimension.\"\"\"\n",
    "    mus = params[:mixtures*dim]\n",
    "    sigs = params[mixtures*dim:2*mixtures*dim]\n",
    "    pis = params[2*mixtures*dim:]\n",
    "    return mus, sigs, pis\n",
    "\n",
    "\n",
    "def adjust_temp(pi_pdf, temp):\n",
    "    \"\"\" Adjusts temperature of a PDF describing a categorical model \"\"\"\n",
    "    pi_pdf = np.log(pi_pdf) / temp\n",
    "    pi_pdf -= pi_pdf.max()\n",
    "    pi_pdf = np.exp(pi_pdf)\n",
    "    pi_pdf /= pi_pdf.sum()\n",
    "    return pi_pdf\n",
    "\n",
    "\n",
    "def get_pi_idx(x, pdf, temp=1.0, greedy=False):\n",
    "    \"\"\"Samples from a categorical model PDF, optionally greedily.\"\"\"\n",
    "    if greedy:\n",
    "        return np.argmax(pdf)\n",
    "    pdf = adjust_temp(np.copy(pdf), temp)\n",
    "    accumulate = 0\n",
    "    for i in range(0, pdf.size):\n",
    "        accumulate += pdf[i]\n",
    "        if accumulate >= x:\n",
    "            return i\n",
    "    tf.logging.info('Error sampling mixture model.')\n",
    "    return -1\n",
    "\n",
    "\n",
    "def sample_from_categorical(dist, temp):\n",
    "    \"\"\"Sample from a categorical model with temperature adjustment.\"\"\"\n",
    "    r = np.random.rand(1)\n",
    "    return get_pi_idx(r,dist,temp)\n",
    "\n",
    "\n",
    "def softmax(w, t=1.0):\n",
    "    \"\"\"Softmax function for a list or numpy array of logits.\"\"\"\n",
    "    e = np.exp(np.array(w) / t)\n",
    "    dist = e / np.sum(e)\n",
    "    return dist\n",
    "\n",
    "# def sample_from_output_1D(params, mixtures, dim, temp=1.0):\n",
    "#     \"\"\"Sample from a 1D MDN output with temperature adjustment.\"\"\"\n",
    "#     mus = params[:mixtures*dim]\n",
    "#     sigs = params[mixtures*dim:2*mixtures*dim]\n",
    "#     pis = params[-mixtures:]\n",
    "#     m = sample_from_categorical(pis, temp=temp)\n",
    "#     return(np.random.normal(mus[m],sigs[m],1))\n",
    "\n",
    "\n",
    "def sample_from_output(params, mixtures, dim, temp=1.0):\n",
    "    \"\"\"Sample from an MDN output with temperature adjustment.\"\"\"\n",
    "    mus = params[:mixtures*dim]\n",
    "    sigs = params[mixtures*dim:2*mixtures*dim]\n",
    "    pis = softmax(params[-mixtures:], t=temp)\n",
    "    m = sample_from_categorical(pis, temp=temp)\n",
    "    mus_vector = mus[m*dim:(m+1)*dim]\n",
    "    sig_vector = sigs[m*dim:(m+1)*dim]\n",
    "    cov_matrix = np.identity(dim) * sig_vector\n",
    "    sample = np.random.multivariate_normal(mus_vector, cov_matrix, 1)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters:\n",
    "SEQ_LEN = 30\n",
    "BATCH_SIZE = 256\n",
    "HIDDEN_UNITS = 64\n",
    "EPOCHS = 100\n",
    "VAL_SPLIT=0.2\n",
    "\n",
    "# These settings train for 2.1 epochs which is pretty good!\n",
    "SEED = 2345  # 2345 seems to be good.\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# tf.set_random_seed(5791)  # only works for current graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "microjam_data_file_name = \"../datasets/TinyPerformanceCorpus.h5\"\n",
    "metatone_data_file_name = \"../datasets/MetatoneTinyPerformanceRecords.h5\"\n",
    "\n",
    "with h5py.File(microjam_data_file_name, 'r') as data_file:\n",
    "    microjam_corpus = data_file['total_performances'][:]\n",
    "with h5py.File(metatone_data_file_name, 'r') as data_file:\n",
    "    metatone_corpus = data_file['total_performances'][:]\n",
    "\n",
    "sequence_loader = robojam.sample_data.SequenceDataLoader(num_steps=SEQ_LEN + 1, batch_size=BATCH_SIZE, corpus=microjam_corpus, overlap=False)\n",
    "# sequence_loader = robojam.sample_data.SequenceDataLoader(num_steps=SEQ_LEN + 1, batch_size=BATCH_SIZE, corpus=metatone_corpus, overlap=False)\n",
    "\n",
    "X, y = sequence_loader.seq_to_singleton_format()\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(\"X:\", X.shape, \"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIMENSION = 3\n",
    "NUMBER_MIXTURES = 5\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(None,SEQ_LEN,OUTPUT_DIMENSION), return_sequences=True))\n",
    "model.add(keras.layers.LSTM(HIDDEN_UNITS))\n",
    "model.add(MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES))\n",
    "model.compile(loss=get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer=keras.optimizers.Adam(), metrics=[get_mixture_mse_accuracy(3,5)])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=VAL_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding Model\n",
    "\n",
    "decoder = keras.Sequential()\n",
    "decoder.add(keras.layers.LSTM(HIDDEN_UNITS, batch_input_shape=(1,1,OUTPUT_DIMENSION), return_sequences=True, stateful=True))\n",
    "decoder.add(keras.layers.LSTM(HIDDEN_UNITS, stateful=True))\n",
    "decoder.add(MDN(OUTPUT_DIMENSION, NUMBER_MIXTURES))\n",
    "decoder.compile(loss=get_mixture_loss_func(OUTPUT_DIMENSION,NUMBER_MIXTURES), optimizer=keras.optimizers.Adam(), metrics=[get_mixture_mse_accuracy(3,5)])\n",
    "decoder.summary()\n",
    "\n",
    "decoder.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_2D(perf_array_to_df(sequence_loader.examples[512]))\n",
    "t = random.randint(0,len(sequence_loader.examples))\n",
    "\n",
    "p = condition_and_generate(decoder,sequence_loader.examples[t], 5)\n",
    "plot_double_2d(perf_array_to_df(sequence_loader.examples[t]), perf_array_to_df(p))\n",
    "decoder.reset_states()\n",
    "# plot_double_2d(in_df, out_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
